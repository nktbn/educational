# Определение токсичных комментариев

## Задача:
- Обучить модель классификации "токсичных" комментариев.
- значение метрики F1 не меньше 0.75.
## Данные:
csv-файл в котором в столбце _text_  содержится текст комментария, а _toxic_ — целевой признак.

## Статус проекта
**завершён**

## Выводы:
Предварительно размеченные данные  были лемматизированы с помощью инструментов библиотеки `nltk`, так-как это одна из библиотек которая поддерживает лемматизацию англоязычных текстов, далее эти тексты были векторизованы с удалением стоп-слов.  
Полученный набор данных был использован при обучении моделей, из которых была выбрана лучшая.  
Лучшая модель показала на тестовых данных значения метрики удовлетворяющие условиям.

## Используемые библиотеки:
`pandas`,  `scikit-learn`, `tqdm`, `nltk`
***
Версия языка:
```
Python 3.11.7
```

Версии пакетов:
```
pandas==1.5.3
scikit-learn==1.2.2
nltk==3.8.1
tqdm==4.65.0

ipwidgets==8.0.4
```
